{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9929a7e9",
   "metadata": {},
   "source": [
    "# NLP-Driven Spam Detection System for SMS and Email\n",
    "\n",
    "This project focuses on building a robust spam detection system using\n",
    "advanced NLP techniques, TF-IDF feature engineering, and machine learning\n",
    "classifiers. \n",
    "\n",
    "The model is designed for both SMS and email spam filtering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286b567",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Spam messages in SMS and emails pose serious risks including phishing,\n",
    "financial fraud, and identity theft. The objective of this project is to\n",
    "classify messages as **Spam** or **Ham** using content-based NLP techniques\n",
    "while maintaining a balance between precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def libraries():\n",
    "    import pandas as pd\n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    nltk.download(\"wordnet\")\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    nltk.download(\"stopwords\")\n",
    "    stop_words = set(stopwords.words(\"English\"))\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import ComplementNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score,classification_report,RocCurveDisplay,confusion_matrix,ConfusionMatrixDisplay\n",
    "    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.pipeline import Pipeline ,FeatureUnion\n",
    "    import joblib\n",
    "    return pd,np,plt,train_test_split,re,nltk,stopwords,WordNetLemmatizer,TfidfVectorizer,sent_tokenize,stop_words,KNeighborsClassifier,SVC,DecisionTreeClassifier,ComplementNB,LogisticRegression,RandomForestClassifier,Pipeline,accuracy_score,f1_score,recall_score,precision_score,GridSearchCV,RandomizedSearchCV,LinearSVC,classification_report,RocCurveDisplay,Pipeline,joblib,sns,confusion_matrix,ConfusionMatrixDisplay,FeatureUnion\n",
    "\n",
    "\n",
    "pd,np,plt,train_test_split,re,nltk,stopwords,WordNetLemmatizer,TfidfVectorizer,sent_tokenize,stop_words,KNeighborsClassifier,SVC,DecisionTreeClassifier,ComplementNB,LogisticRegression,RandomForestClassifier,Pipeline,accuracy_score,f1_score,recall_score,precision_score,GridSearchCV,RandomizedSearchCV,LinearSVC,classification_report,RocCurveDisplay,Pipeline,joblib,sns,confusion_matrix,ConfusionMatrixDisplay,FeatureUnion= libraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58731208",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "- The dataset consists of labeled SMS and email messages.\n",
    "- Classes:\n",
    "  - 0 → Ham (Legitimate messages)\n",
    "  - 1 → Spam (Promotional / Phishing messages)\n",
    "- The dataset is imbalanced, which is handled using class weighting\n",
    "  and threshold tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02445524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    df = pd.read_csv(r\"C:\\Users\\chaud\\Downloads\\spam.csv\",encoding= \"latin\")\n",
    "    df = df[[\"v1\",\"v2\"]]\n",
    "    df.columns = [\"Labels\",\"Messages\"]\n",
    "    df[\"Labels\"] = df[\"Labels\"].map({\"ham\":0,\"spam\":1})\n",
    "    #df.drop_duplicates()\n",
    "    return df\n",
    "df = dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(df):\n",
    "    print(\"---Dataset info---\")\n",
    "    info = df.info()\n",
    "    description = df.describe()\n",
    "    unique_sum = df.nunique()\n",
    "    value_count = df[\"Labels\"].value_counts()if \"Labels\" in df.columns else \"Column 'Labels' not found\"\n",
    "    pie_chart = plt.subplots()\n",
    "    pie_chart = plt.pie(value_count,autopct= \"%1.1f%%\")\n",
    "    pie_chart = plt.show()\n",
    "    return info ,description,unique_sum,value_count,pie_chart\n",
    "info,description,unique_sum,value_count,pie_chart = dataset_info(df) \n",
    "\n",
    "print(\"\\n--- Descriptive Statistics ---\\n\", description)\n",
    "print(\"\\n--- Unique Values per Column ---\\n\", unique_sum)\n",
    "print(\"\\n--- Value Counts for Labels ---\\n\", value_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e166ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_data(df,train_test_split,random_state = 42,test_size = 0.30):\n",
    "    x = df[\"Messages\"]\n",
    "    y = df[\"Labels\"]\n",
    "    x_train , x_test ,y_train ,y_test = train_test_split(x,y,random_state= random_state,stratify=y,test_size=test_size)\n",
    "    return x_train , x_test ,y_train ,y_test ,x,y\n",
    "x_train , x_test ,y_train ,y_test,x,y  = splitting_data(df,train_test_split,random_state = 42,test_size = 0.30)\n",
    "print(f\"Training sample :\",len(x_train))\n",
    "print(f\"Testing sample :\",len(x_test))\n",
    "print(\"\\n--- x_train ---\\n\",x_train)\n",
    "print(\"\\n--- x_train_type ---\\n\",type(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44adf7",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "The following preprocessing steps are applied:\n",
    "\n",
    "- Lowercasing text\n",
    "- URL detection and removal (with signal preservation)\n",
    "- Semantic feature injection (urgency, brand mention, link presence)\n",
    "- Number normalization\n",
    "- Lemmatization\n",
    "- Stopword removal\n",
    "\n",
    "Instead of adding manual numerical features, semantic indicators are injected\n",
    "directly into the text so that TF-IDF can learn them naturally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca63ae7",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "To improve phishing and promotional spam detection, the following semantic\n",
    "features are injected into the text:\n",
    "\n",
    "- `link_found` → Detects presence of URLs\n",
    "- `urgency_flag` → Captures urgency language such as \"urgent\", \"verify\"\n",
    "- `brand_mention` → Detects brand impersonation (Netflix, PayPal, Amazon)\n",
    "\n",
    "Both **word-level TF-IDF** and **character-level TF-IDF** are used to capture\n",
    "lexical as well as morphological spam patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e699c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(message,stop_words):\n",
    "    message = str(message).lower()\n",
    "    def add_custom_feature(message):\n",
    "        if \"http\" in message or \"www\" in message:\n",
    "            message = message + \" link_found\"\n",
    "        if any(word in message for word in [\"urgent\", \"verify\", \"action required\"]):\n",
    "            message += \" urgency_flag\"\n",
    "        brands = [\"netflix\",\"amazon\",\"google\",\"paypal\"]    \n",
    "        if any(brand in message for brand in brands): \n",
    "            message += \" brand_mention\"   \n",
    "        if \"verify\" in message and \"account\" in message:\n",
    "            message += \" phishing_pattern\"\n",
    "        if \"action required\" in message:\n",
    "            message += \" phishing_pattern\"\n",
    "        return message\n",
    "    message = add_custom_feature(message)\n",
    "    message = re.sub(r\"http\\S+|www\\.\\S+\", \"\", message)\n",
    "    message = re.sub(r'\\d+', ' number_token ', message)\n",
    "    message = re.sub('[^a-zA-Z0-9!£$% ]', \" \", message)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = message.split()\n",
    "    cleaned_words = [lemmatizer.lemmatize(word)for word in words if word not in stop_words]\n",
    "    return \" \".join(cleaned_words)\n",
    "     \n",
    "x_train_clean = x_train.apply(lambda msg: preprocessing(msg,stop_words))\n",
    "x_test_clean = x_test.apply(lambda msg: preprocessing(msg,stop_words))    \n",
    "print(\"\\n----cleaned training dataset----\")\n",
    "print(x_train_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389caa2b",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "\n",
    "- Word-level TF-IDF:\n",
    "  - Unigrams and bigrams\n",
    "  - Stopword removal\n",
    "- Character-level TF-IDF:\n",
    "  - Character n-grams (3–5)\n",
    "\n",
    "This hybrid approach improves robustness against obfuscated spam text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(x_train_clean,x_test_clean,TfidfVectorizer):\n",
    "    word_tfidf = TfidfVectorizer(analyzer=\"word\",max_features = 5000 ,stop_words= \"english\",ngram_range=(1,2),min_df=5, max_df= 0.9,sublinear_tf= True)\n",
    "    char_tfidf = TfidfVectorizer(analyzer= \"char\",ngram_range=(3,5),max_features = 3000)\n",
    "    vectorizer = FeatureUnion([(\"word_tfidf\",word_tfidf),(\"char_tfidf\",char_tfidf)])\n",
    "    x_train_clean_vec = vectorizer.fit_transform(x_train_clean)\n",
    "    x_test_clean_vec = vectorizer.transform(x_test_clean)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return x_train_clean_vec,x_test_clean_vec,vectorizer,feature_names    \n",
    "\n",
    "x_train_clean_vec,x_test_clean_vec,vectorizer,feature_names =  vectorization(x_train_clean,x_test_clean,TfidfVectorizer)   \n",
    "print(f\"Vectorizer :\",vectorizer)\n",
    "print(f\"x_train_vec :\",x_train_clean_vec)\n",
    "print(f\"x_test_vec :\",x_test_clean_vec)\n",
    "print(f\"Feature_names :\",feature_names)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717178ef",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Support Vector Machine (SVM) with a linear kernel is used due to its strong\n",
    "performance on high-dimensional sparse text data.\n",
    "\n",
    "Class imbalance is handled using `class_weight='balanced'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756efc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_select(x_train_clean_vec,x_test_clean_vec , y_train,y_test):\n",
    "    models = {\n",
    "        \"SVC\":SVC(class_weight=\"balanced\",probability=True),\n",
    "        \"DecisionTreeClassifier\":DecisionTreeClassifier(class_weight = \"balanced\"),\n",
    "        \"ComplementNB\":ComplementNB(),\n",
    "        \"LogisticRegression\":LogisticRegression(max_iter= 1000,class_weight= \"balanced\"),\n",
    "        \"RandomForestClassifier\":RandomForestClassifier(class_weight= \"balanced\"),\n",
    "        \"KNeighborsClassifier\":KNeighborsClassifier(weights = \"distance\")\n",
    "    }\n",
    "    pipelines_dict = {}\n",
    "    predictions = {}\n",
    "    for name,model in models.items():\n",
    "        pipe = Pipeline([\n",
    "        (\"clf\",model)])\n",
    "        pipe.fit(x_train_clean_vec,y_train)\n",
    "        y_pred = pipe.predict(x_test_clean_vec)\n",
    "\n",
    "        metrics_model = {\n",
    "            \"accuracy_score\":accuracy_score(y_test,y_pred),\n",
    "            \"f1_score\":f1_score(y_test,y_pred,average=\"weighted\"),\n",
    "            \"recall_score\":recall_score(y_test,y_pred,average=\"weighted\"),\n",
    "            \"precision_score\":precision_score(y_test,y_pred,average=\"weighted\")\n",
    "        }\n",
    "        pipelines_dict[name] = pipe\n",
    "        predictions[name] = metrics_model\n",
    "    best_model_name = max(predictions, key= lambda z:predictions[z][\"f1_score\"])\n",
    "    return pipelines_dict, predictions, best_model_name\n",
    "   \n",
    "\n",
    "pipelines_dict, predictions, best_model_name = model_select(x_train_clean_vec,x_test_clean_vec,y_train,y_test)\n",
    "print(f\"pipelines :\",{Pipeline})\n",
    "print(f\"predictions :\",predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0556e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a48ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter(x_train_clean_vec,y_train):\n",
    "    \n",
    "    #svc with grid search cv\n",
    "    param_grid = {\n",
    "        \"C\": [0.001,0.01,1,10,50,100],\n",
    "        \"kernel\":[\"linear\"],\n",
    "        \"gamma\":[\"scale\", \"auto\"], \n",
    "    }\n",
    "    Grid_Search_CV = GridSearchCV(estimator= SVC(probability= True,class_weight= \"balanced\"),param_grid= param_grid,cv=5,scoring= \"f1_weighted\",n_jobs=-1)\n",
    "    Grid_Search_CV.fit(x_train_clean_vec,y_train)\n",
    "    #linearsvc with random search cv\n",
    "\n",
    "    param_distributions = { \n",
    "        \"C\": np.logspace(-3, 2, 20)\n",
    "        }\n",
    "    Randomized_Search_CV = RandomizedSearchCV(estimator= SVC(kernel='linear', class_weight='balanced', probability=True),param_distributions = param_distributions,cv=5,n_jobs=-1,scoring= \"f1_weighted\")\n",
    "    Randomized_Search_CV.fit(x_train_clean_vec,y_train)\n",
    "\n",
    "    return Grid_Search_CV ,Randomized_Search_CV\n",
    "\n",
    "Grid_Search_CV,Randomized_Search_CV= hyperparameter(x_train_clean_vec,y_train) \n",
    "print(\"------------- SVC WITH GRIDSEARCHCV-------------------------------------------------------------------------------------\")\n",
    "print(\"Grid_Search_CV:\",Grid_Search_CV)\n",
    "print(\"Best Params:\", Grid_Search_CV.best_params_)\n",
    "print(\"Best_estimator:\",Grid_Search_CV.best_estimator_)\n",
    "print(\"Best F1:\", Grid_Search_CV.best_score_)\n",
    "print(\"------------- LINEARSVC WITH RANDOMIZEDSEARCHCV-------------------------------------------------------------------------------------\")\n",
    "print(\"Randomized_Search_CV:\",Randomized_Search_CV)\n",
    "print(\"Best Params:\", Randomized_Search_CV.best_params_)\n",
    "print(\"Best_estimator:\",Randomized_Search_CV.best_estimator_)\n",
    "print(\"Best F1:\", Randomized_Search_CV.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_hyperparameter_name(Randomized_Search_CV,x_train_clean_vec,x_test_clean_vec,y_train,y_test):\n",
    "    y_pred_train = Randomized_Search_CV.predict(x_train_clean_vec)\n",
    "    y_pred_test = Randomized_Search_CV.predict(x_test_clean_vec)   \n",
    "    return y_pred_train,y_pred_test\n",
    "y_pred_train,y_pred_test = best_model_hyperparameter_name(Randomized_Search_CV,x_train_clean_vec,x_test_clean_vec,y_train,y_test)\n",
    "print(\"------------------- FOR TRAINING DATA--------------------------------------------------------------------------------------\")\n",
    "print(\"accuracy_score:\",accuracy_score(y_pred_train,y_train))\n",
    "print(\"f1_weighted:\",f1_score(y_pred_train,y_train,average = \"weighted\"))\n",
    "\n",
    "print(\"------------------- FOR TESTING DATA---------------------------------------------------------------------------------------\")\n",
    "print(\"accuracy_score:\",accuracy_score(y_pred_test,y_test))\n",
    "print(\"f1_weighted:\",f1_score(y_pred_test,y_test,average = \"weighted\"))\n",
    "print(\"\\n full classification report of testing set:\\n\",classification_report(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee75a6",
   "metadata": {},
   "source": [
    "FOR UNIGRAM | NGRAM_RANGE = (1,1)\n",
    "\n",
    "------------------- FOR TRAINING DATA--------------------------------------------------------------------------------------\n",
    "\n",
    "accuracy_score: 1.0\n",
    "\n",
    "f1_weighted: 1.0\n",
    "\n",
    "------------------- FOR TESTING DATA---------------------------------------------------------------------------------------\n",
    "\n",
    "accuracy_score: 0.9814593301435407\n",
    "\n",
    "f1_weighted: 0.9816576008685878\n",
    "\n",
    "\n",
    " full classification report of testing set:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99      1459\n",
    "           1       0.91      0.95      0.93       213\n",
    "\n",
    "    accuracy                           0.98      1672\n",
    "   macro avg       0.95      0.97      0.96      1672\n",
    "weighted avg       0.98      0.98      0.98      1672\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89828eed",
   "metadata": {},
   "source": [
    "FOR UNIGRAMS,BIGRAMS | NGRAM_RANGE = (1,2)\n",
    "\n",
    "------------------- FOR TRAINING DATA--------------------------------------------------------------------------------------\n",
    "\n",
    "accuracy_score: 0.9994871794871795\n",
    "\n",
    "f1_weighted: 0.9994867661055344\n",
    "\n",
    "\n",
    "------------------- FOR TESTING DATA---------------------------------------------------------------------------------------\n",
    "\n",
    "accuracy_score: 0.979066985645933\n",
    "\n",
    "f1_weighted: 0.9791670782995956\n",
    "\n",
    "\n",
    " full classification report of testing set:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99      1453\n",
    "           1       0.91      0.93      0.92       219\n",
    "\n",
    "    accuracy                           0.98      1672\n",
    "   macro avg       0.95      0.96      0.95      1672\n",
    "weighted avg       0.98      0.98      0.98      1672"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9097b09",
   "metadata": {},
   "source": [
    "FOR BIGRAM | NGRAM_RANGE = (2,2)\n",
    "\n",
    "------------------- FOR TRAINING DATA--------------------------------------------------------------------------------------\n",
    "\n",
    "accuracy_score: 0.9994871794871795\n",
    "\n",
    "f1_weighted: 0.9994867661055344\n",
    "\n",
    "------------------- FOR TESTING DATA---------------------------------------------------------------------------------------\n",
    "\n",
    "accuracy_score: 0.979066985645933\n",
    "\n",
    "f1_weighted: 0.9791670782995956\n",
    "\n",
    " full classification report of testing set:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99      1453\n",
    "           1       0.91      0.93      0.92       219\n",
    "\n",
    "    accuracy                           0.98      1672\n",
    "   macro avg       0.95      0.96      0.95      1672\n",
    "weighted avg       0.98      0.98      0.98      1672\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(x_test_clean_vec,y_test):\n",
    "    curve = RocCurveDisplay.from_estimator(Randomized_Search_CV.best_estimator_, x_test_clean_vec,y_test)\n",
    "    curve = plt.show()\n",
    "    return curve\n",
    "curve = roc_curve(x_test_clean_vec,y_test)\n",
    "curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfae372",
   "metadata": {},
   "source": [
    "## Threshold Optimization\n",
    "\n",
    "Instead of using the default threshold (0.5), probability-based threshold\n",
    "tuning is applied.\n",
    "\n",
    "- A threshold of **0.35** is selected to maximize spam recall\n",
    "- This reduces false negatives while keeping false positives minimal\n",
    "\n",
    "Threshold tuning allows the model to adapt to different deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9a576",
   "metadata": {},
   "source": [
    "## Model Evaluation (Threshold = 0.35)\n",
    "\n",
    "The confusion matrix obtained at threshold = 0.35 is shown below:\n",
    "\n",
    "- True Negatives (Ham correctly identified): 1442\n",
    "- False Positives (Ham misclassified as Spam): 6\n",
    "- False Negatives (Spam missed): 16\n",
    "- True Positives (Spam correctly identified): 208\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_pipeline(x_train,y_train):\n",
    "    word_tfidf = TfidfVectorizer(analyzer=\"word\",max_features = 5000 ,stop_words= \"english\",ngram_range=(1,2),min_df=5, max_df= 0.9,sublinear_tf= True)\n",
    "    char_tfidf = TfidfVectorizer(analyzer= \"char\",ngram_range=(3,5),max_features = 3000)\n",
    "    vectorizer = FeatureUnion([(\"word_tfidf\",word_tfidf),(\"char_tfidf\",char_tfidf)])\n",
    "    pipeline_for_deployment = Pipeline([\n",
    "        (\"Vectorizer\",vectorizer),\n",
    "        (\"Classifier\",Randomized_Search_CV.best_estimator_)\n",
    "    ])\n",
    "    final_pipeline = pipeline_for_deployment.fit(x_train,y_train)\n",
    "    probs = final_pipeline.predict_proba(x_test)[:, 1]\n",
    "    y_pred = (probs > 0.35).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Ham', 'Spam'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(\"Spam Detection Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    results = pd.DataFrame({'text': x_test, 'actual': y_test, 'predicted': y_pred})\n",
    "    dump_model = joblib.dump(final_pipeline,\"spam.pkl\")\n",
    "    return pipeline_for_deployment,final_pipeline,dump_model,results\n",
    "pipeline_for_deployment,final_pipeline,dump_model,results = deploy_pipeline(x_train,y_train) \n",
    "print(\"✅ Pipeline Created with Preprocessor\")\n",
    "print(\"✅ Best Model Integrated:\", Randomized_Search_CV.best_params_)\n",
    "print(\"✅ Best Model :\", Randomized_Search_CV.best_estimator_)\n",
    "print(\"✅ Model Dumped to:\", dump_model)\n",
    "print(\"--- Ham marked as Spam (Critical Errors) ---\")\n",
    "display(results[(results['actual'] == 0) & (results['predicted'] == 1)])\n",
    "print(\"\\n--- Spam marked as Ham (Missed Spam) ---\")\n",
    "display(results[(results['actual'] == 1) & (results['predicted'] == 0)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de654ff",
   "metadata": {},
   "source": [
    "Lowering the threshold from 0.90 to 0.35 improved recall significantly with only a minor increase in false positives.\n",
    "\n",
    "For consumer SMS and email systems, this trade-off maximizes user protection while maintaining high precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5aa278",
   "metadata": {},
   "source": [
    "## Test Case Evaluation\n",
    "\n",
    "The model was tested on real-world SMS and email examples including:\n",
    "\n",
    "- Casual conversations\n",
    "- Promotional spam\n",
    "- Phishing attempts\n",
    "- Brand impersonation messages\n",
    "\n",
    "This evaluation ensures robustness beyond the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a27a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    model = joblib.load('spam.pkl')\n",
    "    test_messages = [\n",
    "    \"Hey, are we still meeting for coffee at 4?\",\n",
    "    \"Can you send me the notes from today's meeting?\",\n",
    "    \"I'll be home a bit late, don't wait up for dinner.\",\n",
    "    \"WINNER! You have won a $1000 Walmart gift card. Click here to claim now!\",\n",
    "    \"URGENT: Your account has been compromised. Log in at http://bit.ly/fake-link to reset.\",\n",
    "    \"FREE entry into our £100 weekly draw. Text 'WIN' to 80085 to join.\",\n",
    "    \"Please call me back regarding your insurance claim.\",\n",
    "    \"I'm at the bank right now, will wire the money soon.\", \n",
    "    \"CONGRATS on the new job! Let's celebrate!\",\n",
    "    \"Action Required: Your Netflix subscription has expired.\",\n",
    "    \"The invoice for the project is attached.\",\n",
    "    \"Yo, you coming to the gym?\",\n",
    "    \"I won the local chess tournament today!\",\n",
    "    \"Verify your identity within 24 hours.\",\"Hey, are we still meeting for coffee at 4?\",\n",
    "    \"Can you send me the notes from today's meeting?\",\n",
    "    \"I'll be home a bit late, don't wait up for dinner.\",\n",
    "    \"Yo, you coming to the gym?\",\n",
    "    \"I won the local chess tournament today!\",\n",
    "    \"The invoice for the project is attached.\",\n",
    "    \"CONGRATS on the new job! Let's celebrate!\",\n",
    "    \"I'm at the bank right now, will wire the money soon.\",\n",
    "    \"WINNER! You have won a $1000 Walmart gift card. Click here to claim now!\",\n",
    "    \"FREE entry into our £100 weekly draw. Text 'WIN' to 80085 to join.\",\n",
    "    \"Congratulations! Claim your reward now before it expires.\",\n",
    "    \"URGENT: Your account has been compromised. Log in at http://bit.ly/fake-link to reset.\",\n",
    "    \"Verify your identity within 24 hours to avoid suspension.\",\n",
    "    \"Action Required: Your Netflix subscription has expired.\",\n",
    "    \"Your PayPal account is limited. Verify now to restore access.\",\n",
    "    \"Please call me back regarding your insurance claim.\",\n",
    "    \"Your Amazon order has been shipped. Track here.\",\n",
    "    \"Reminder: Bank KYC update required within 24 hours.\"]\n",
    "    predictions = model.predict(test_messages)\n",
    "    return model,predictions\n",
    "model,predictions  = testing()\n",
    "print(f\"Model : {model}\")\n",
    "print(f\"Predictions : {predictions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d7a5f",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Some phishing messages without explicit URLs or sender metadata were\n",
    "classified as ham. This behavior mirrors real-world spam filters and\n",
    "highlights the limitation of content-only models.\n",
    "\n",
    "Future improvements can include sender reputation and email header analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4d3cb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project demonstrates an effective NLP-based spam detection system using\n",
    "TF-IDF feature engineering, SVM classification, and threshold tuning.\n",
    "\n",
    "The system achieves high precision and recall while maintaining realistic\n",
    "behavior suitable for deployment in SMS and email platforms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046fb5de",
   "metadata": {},
   "source": [
    "## Key Takeaway\n",
    "\n",
    "Threshold tuning and semantic feature injection significantly improve spam\n",
    "detection performance while preserving user experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c477e0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2424b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a7492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vanshunlpproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
